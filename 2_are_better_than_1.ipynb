{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Colab_25GBRAM_GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saarahasad/machine_learning_examples/blob/master/2_are_better_than_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdHVQrqyDB50"
      },
      "source": [
        "# Memory Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E58HaeA6CsM9"
      },
      "source": [
        "import psutil\n",
        "def get_size(bytes, suffix=\"B\"):\n",
        "    factor = 1024\n",
        "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
        "        if bytes < factor:\n",
        "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
        "        bytes /= factor\n",
        "print(\"=\"*40, \"Memory Information\", \"=\"*40)\n",
        "svmem = psutil.virtual_memory()\n",
        "print(f\"Total: {get_size(svmem.total)}\") ; print(f\"Available: {get_size(svmem.available)}\")\n",
        "print(f\"Used: {get_size(svmem.used)}\") ; print(f\"Percentage: {svmem.percent}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgVfBlvZDEja"
      },
      "source": [
        "# GPU Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxlkxvkrCyin",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "153cb027-8ccb-46fe-d8f9-81a5e60f4a50"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Mar 28 10:50:36 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyDODoaWC6KI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "57a2da86-d362-480b-b12d-d9548d9a2106"
      },
      "source": [
        "!unzip two-are-better-than-one-master.zip\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  two-are-better-than-one-master.zip\n",
            "670087aa17dbe0e93c10a810bd0fcd25bef29478\n",
            "   creating: two-are-better-than-one-master/\n",
            "  inflating: two-are-better-than-one-master/README.md  \n",
            "   creating: two-are-better-than-one-master/data/\n",
            "   creating: two-are-better-than-one-master/data/.ipynb_checkpoints/\n",
            "  inflating: two-are-better-than-one-master/data/.ipynb_checkpoints/__init__-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/data/.ipynb_checkpoints/base-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/data/.ipynb_checkpoints/basics-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/data/__init__.py  \n",
            "   creating: two-are-better-than-one-master/data/__pycache__/\n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/base.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/base.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/basics.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/basics.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/binary_matrix_data.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/binary_matrix_data.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/few_slu_data.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/joint_data.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/lm_pyramid_nest_ner_data.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/multi_binary_matrix_data.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/multi_binary_matrix_data.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/nest_ner_data.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/pyramid_nest_ner_data.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/slot_filling_data.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/slot_filling_data.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/slu_data.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/slu_data.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/__pycache__/twine_data.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/data/base.py  \n",
            "  inflating: two-are-better-than-one-master/data/basics.py  \n",
            "  inflating: two-are-better-than-one-master/data/joint_data.py  \n",
            "   creating: two-are-better-than-one-master/datasets/\n",
            "  inflating: two-are-better-than-one-master/datasets/README.md  \n",
            "   creating: two-are-better-than-one-master/datasets/ace2004/\n",
            "  inflating: two-are-better-than-one-master/datasets/ace2004/ace2ann.py  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2004/ace2json.py  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2004/run.zsh  \n",
            "   creating: two-are-better-than-one-master/datasets/ace2004/split/\n",
            "  inflating: two-are-better-than-one-master/datasets/ace2004/split/cv0  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2004/split/cv1  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2004/split/cv2  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2004/split/cv3  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2004/split/cv4  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2004/train_list  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2004/train_list_fixed  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2004/unify.py  \n",
            "   creating: two-are-better-than-one-master/datasets/ace2005/\n",
            "  inflating: two-are-better-than-one-master/datasets/ace2005/ace2ann.py  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2005/ace2json.py  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2005/run.zsh  \n",
            "   creating: two-are-better-than-one-master/datasets/ace2005/split/\n",
            "  inflating: two-are-better-than-one-master/datasets/ace2005/split/dev  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2005/split/test  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2005/split/train  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2005/train_list  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2005/train_list_fixed  \n",
            "  inflating: two-are-better-than-one-master/datasets/ace2005/unify.py  \n",
            "   creating: two-are-better-than-one-master/datasets/common/\n",
            "  inflating: two-are-better-than-one-master/datasets/common/conll2txt.py  \n",
            "  inflating: two-are-better-than-one-master/datasets/common/dep2so.prl  \n",
            "  inflating: two-are-better-than-one-master/datasets/common/fix_sentence_break.py  \n",
            "  inflating: two-are-better-than-one-master/datasets/common/props_fixed  \n",
            "  inflating: two-are-better-than-one-master/datasets/common/props_ssplit  \n",
            "  inflating: two-are-better-than-one-master/datasets/common/standoff.py  \n",
            "   creating: two-are-better-than-one-master/datasets/unified/\n",
            "  inflating: two-are-better-than-one-master/datasets/unified/test.ADE0.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/test.ADE1.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/test.ADE2.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/test.ADE3.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/test.ADE4.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/test.ADE5.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/test.ADE6.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/test.ADE7.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/test.ADE8.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/test.ADE9.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/test.CoNLL04.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/train.ADE0.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/train.ADE1.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/train.ADE2.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/train.ADE3.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/train.ADE4.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/train.ADE5.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/train.ADE6.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/train.ADE7.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/train.ADE8.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/train.ADE9.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/train.CoNLL04.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/valid.ADE0.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/valid.ADE1.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/valid.ADE2.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/valid.ADE3.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/valid.ADE4.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/valid.ADE5.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/valid.ADE6.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/valid.ADE7.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/valid.ADE8.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/valid.ADE9.json  \n",
            "  inflating: two-are-better-than-one-master/datasets/unified/valid.CoNLL04.json  \n",
            "   creating: two-are-better-than-one-master/functions/\n",
            "   creating: two-are-better-than-one-master/functions/.ipynb_checkpoints/\n",
            "  inflating: two-are-better-than-one-master/functions/.ipynb_checkpoints/__init__-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/functions/.ipynb_checkpoints/activations-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/functions/.ipynb_checkpoints/generals-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/functions/.ipynb_checkpoints/initializations-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/functions/.ipynb_checkpoints/metrics-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/functions/__init__.py  \n",
            "   creating: two-are-better-than-one-master/functions/__pycache__/\n",
            "  inflating: two-are-better-than-one-master/functions/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/functions/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/functions/__pycache__/activations.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/functions/__pycache__/activations.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/functions/__pycache__/generals.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/functions/__pycache__/generals.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/functions/__pycache__/initializations.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/functions/__pycache__/initializations.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/functions/__pycache__/metrics.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/functions/__pycache__/metrics.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/functions/__pycache__/multi_head_attentions.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/functions/__pycache__/multi_head_attentions.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/functions/activations.py  \n",
            "  inflating: two-are-better-than-one-master/functions/generals.py  \n",
            "  inflating: two-are-better-than-one-master/functions/initializations.py  \n",
            "  inflating: two-are-better-than-one-master/functions/metrics.py  \n",
            "  inflating: two-are-better-than-one-master/functions/multi_head_attentions.py  \n",
            "   creating: two-are-better-than-one-master/gens/\n",
            "  inflating: two-are-better-than-one-master/gens/gen_bert.py  \n",
            "  inflating: two-are-better-than-one-master/gens/gen_elmo.py  \n",
            "  inflating: two-are-better-than-one-master/gens/gen_roberta.py  \n",
            "  inflating: two-are-better-than-one-master/inference.ipynb  \n",
            "   creating: two-are-better-than-one-master/layers/\n",
            "   creating: two-are-better-than-one-master/layers/.ipynb_checkpoints/\n",
            "  inflating: two-are-better-than-one-master/layers/.ipynb_checkpoints/__init__-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/layers/.ipynb_checkpoints/crfs-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/layers/.ipynb_checkpoints/indexings-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/layers/.ipynb_checkpoints/losses-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/layers/__init__.py  \n",
            "   creating: two-are-better-than-one-master/layers/__pycache__/\n",
            "  inflating: two-are-better-than-one-master/layers/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/__pycache__/activations.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/__pycache__/activations.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/__pycache__/crfs.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/__pycache__/crfs.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/__pycache__/indexings.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/__pycache__/indexings.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/__pycache__/losses.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/__pycache__/losses.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/__pycache__/others.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/__pycache__/others.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/activations.py  \n",
            "  inflating: two-are-better-than-one-master/layers/crfs.py  \n",
            "   creating: two-are-better-than-one-master/layers/encodings/\n",
            "   creating: two-are-better-than-one-master/layers/encodings/.ipynb_checkpoints/\n",
            "  inflating: two-are-better-than-one-master/layers/encodings/.ipynb_checkpoints/__init__-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/.ipynb_checkpoints/routings-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/.ipynb_checkpoints/seqs-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__init__.py  \n",
            "   creating: two-are-better-than-one-master/layers/encodings/__pycache__/\n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/attentions.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/attentions.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/embeddings.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/embeddings.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/gates.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/gates.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/lm_embeddings.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/lm_embeddings.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/lstm_2d.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/matrices.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/matrices.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/mixes.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/mixes.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/pretrained.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/rnn25d.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/rnn2d.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/rnn2d.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/routings.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/routings.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/seq2mat.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/seq2mat.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/seqs.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/__pycache__/seqs.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/attentions.py  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/embeddings.py  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/gates.py  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/lm_embeddings.py  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/matrices.py  \n",
            "   creating: two-are-better-than-one-master/layers/encodings/mdrnns/\n",
            "  inflating: two-are-better-than-one-master/layers/encodings/mdrnns/__init__.py  \n",
            "   creating: two-are-better-than-one-master/layers/encodings/mdrnns/__pycache__/\n",
            "  inflating: two-are-better-than-one-master/layers/encodings/mdrnns/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/mdrnns/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/mdrnns/__pycache__/gru.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/mdrnns/__pycache__/gru.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/mdrnns/__pycache__/lstm.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/mdrnns/__pycache__/lstm.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/mdrnns/gru.py  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/routings.py  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/seq2mat.py  \n",
            "  inflating: two-are-better-than-one-master/layers/encodings/seqs.py  \n",
            "  inflating: two-are-better-than-one-master/layers/indexings.py  \n",
            "  inflating: two-are-better-than-one-master/layers/losses.py  \n",
            "  inflating: two-are-better-than-one-master/layers/others.py  \n",
            "   creating: two-are-better-than-one-master/models/\n",
            "  inflating: two-are-better-than-one-master/models/__init__.py  \n",
            "   creating: two-are-better-than-one-master/models/__pycache__/\n",
            "  inflating: two-are-better-than-one-master/models/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/models/__pycache__/base.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/models/__pycache__/joint_models.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/models/base.py  \n",
            "  inflating: two-are-better-than-one-master/models/joint_models.py  \n",
            "  inflating: two-are-better-than-one-master/train.py  \n",
            "   creating: two-are-better-than-one-master/utils/\n",
            "   creating: two-are-better-than-one-master/utils/.ipynb_checkpoints/\n",
            "  inflating: two-are-better-than-one-master/utils/.ipynb_checkpoints/__init__-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/utils/.ipynb_checkpoints/conveniences-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/utils/.ipynb_checkpoints/cuda-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/utils/.ipynb_checkpoints/data-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/utils/.ipynb_checkpoints/decorators-checkpoint.py  \n",
            "  inflating: two-are-better-than-one-master/utils/__init__.py  \n",
            "   creating: two-are-better-than-one-master/utils/__pycache__/\n",
            "  inflating: two-are-better-than-one-master/utils/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/utils/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/utils/__pycache__/conveniences.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/utils/__pycache__/conveniences.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/utils/__pycache__/cuda.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/utils/__pycache__/cuda.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/utils/__pycache__/data.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/utils/__pycache__/data.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/utils/__pycache__/decorators.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/utils/__pycache__/decorators.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/utils/__pycache__/evaluations.cpython-36.pyc  \n",
            "  inflating: two-are-better-than-one-master/utils/__pycache__/evaluations.cpython-37.pyc  \n",
            "  inflating: two-are-better-than-one-master/utils/conveniences.py  \n",
            "  inflating: two-are-better-than-one-master/utils/cuda.py  \n",
            "  inflating: two-are-better-than-one-master/utils/data.py  \n",
            "  inflating: two-are-better-than-one-master/utils/decorators.py  \n",
            "  inflating: two-are-better-than-one-master/utils/evaluations.py  \n",
            "   creating: two-are-better-than-one-master/wv/\n",
            "  inflating: two-are-better-than-one-master/wv/glove.6B.100d.ace04.txt  \n",
            "  inflating: two-are-better-than-one-master/wv/glove.6B.100d.ace05.txt  \n",
            "  inflating: two-are-better-than-one-master/wv/glove.6B.100d.ade.txt  \n",
            "  inflating: two-are-better-than-one-master/wv/glove.6B.100d.conll04.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hrilQrPfeApz",
        "outputId": "d9c6943d-429b-4564-a788-943cccb5e76c"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 41.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=8314d3fbf39e904026a76f04819b3e2ac4badd182c6b150a226898b25e220a0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Psz9-dGjezO9",
        "outputId": "ef500bd2-6000-4839-c642-26d0c9a74336"
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/3a/1b46a0220d6176b22bcb9336619d1731301bc2c75fa926a9ef953e6e4d58/flair-0.8.0.post1-py3-none-any.whl (284kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 8.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.95\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Collecting huggingface-hub\n",
            "  Downloading https://files.pythonhosted.org/packages/af/07/bf95f398e6598202d878332280f36e589512174882536eb20d792532a57d/huggingface_hub-0.0.7-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.4MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 34.2MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 40.8MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 19.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.4.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/02/be/4dd30d56a0a19619deb9bf41ba8202709fa83b1b301b876572cd6dc38117/konoha-4.6.4-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Collecting torch<=1.7.1,>=1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 22kB/s \n",
            "\u001b[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading https://files.pythonhosted.org/packages/50/21/92c3cfe56f5c0647145c4b0083d0733dd4890a057eb100a8eeddf949ffe9/gdown-3.12.2.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/73/994edfcba74443146c84b91921fcc269374354118d4f452fb0c54c1cbb12/Deprecated-1.2.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.6.1->flair) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (20.9)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.7.1,>=1.5.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (4.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub->flair) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-cp37-none-any.whl size=9693 sha256=13931dd1f07c594a9729674dd2f3e9956c6153fbe670151997aecf036a5b8b5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/d0/d7/d9983facc6f2775411803e0e2d30ebf98efbf2fc6e57701e09\n",
            "Successfully built gdown\n",
            "Building wheels for collected packages: ftfy, sqlitedict, langdetect, mpld3, segtok, overrides\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp37-none-any.whl size=46451 sha256=5b6fbb5d6c95e7b1fe756cbec53eb9af6f0d043b125ea0a72e6cbd5e112691b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp37-none-any.whl size=14376 sha256=bd94ff705b57d4a2e770e27f89f43c986b749c2865a4dbbd7d6744cfaa23a37e\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp37-none-any.whl size=993193 sha256=da9771e6a7f690e9e042a5fc01b40e46cfb77f3fdfd8e8c1d7ba349f3859afc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp37-none-any.whl size=116679 sha256=20317a84e12ca84749c56ac02cb023f9f53b3565af9a06444b0a768e5aed19b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=9dec8c9cf68b2d71255326bea324180ba5d1779358b6be8d5be530e4a91518d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=b068a85a70f7912153defcf561013eda490a9f5fc22ce70439a1b57b50b4eb9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "Successfully built ftfy sqlitedict langdetect mpld3 segtok overrides\n",
            "\u001b[31mERROR: torchvision 0.9.0+cu101 has requirement torch==1.8.0, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: konoha 4.6.4 has requirement requests<3.0.0,>=2.25.1, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, huggingface-hub, ftfy, janome, sqlitedict, langdetect, mpld3, overrides, konoha, torch, bpemb, segtok, gdown, deprecated, flair\n",
            "  Found existing installation: torch 1.8.0+cu101\n",
            "    Uninstalling torch-1.8.0+cu101:\n",
            "      Successfully uninstalled torch-1.8.0+cu101\n",
            "  Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "Successfully installed bpemb-0.3.2 deprecated-1.2.12 flair-0.8.0.post1 ftfy-5.9 gdown-3.12.2 huggingface-hub-0.0.7 janome-0.4.1 konoha-4.6.4 langdetect-1.0.8 mpld3-0.3 overrides-3.1.0 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 torch-1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nip2y_SCedCK",
        "outputId": "1577d792-9728-490b-a70e-4906b77efa1e"
      },
      "source": [
        "!python /content/two-are-better-than-one-master/gens/gen_bert.py \\\n",
        "    --model albert-xxlarge-v1 \\\n",
        "    --dataset CoNLL04\\\n",
        "    --save_attention 1 \\\n",
        "    --save_path /content/two-are-better-than-one-master/wv/albert.conll04_with_heads.pkl "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-28 11:00:35.161366: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "/content/two-are-better-than-one-master/gens/gen_bert.py:62: DeprecationWarning: Call to deprecated method __init__. (Use 'TransformerWordEmbeddings' for all transformer-based word embeddings) -- Deprecated since version 0.4.5.\n",
            "  super().__init__()\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 921kB/s]\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 23.4kB/s]\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 1.47MB/s]\n",
            "Downloading: 100% 433/433 [00:00<00:00, 338kB/s]\n",
            "Downloading: 100% 440M/440M [00:09<00:00, 46.0MB/s]\n",
            "Downloading: 100% 760k/760k [00:00<00:00, 2.35MB/s]\n",
            "Downloading: 100% 1.31M/1.31M [00:00<00:00, 3.36MB/s]\n",
            "Downloading: 100% 706/706 [00:00<00:00, 704kB/s]\n",
            "Downloading: 100% 893M/893M [00:19<00:00, 46.2MB/s]\n",
            "100% 1441/1441 [02:35<00:00,  9.28it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vJg_s4SPhLo2",
        "outputId": "cd627892-f9d9-489f-dac1-334ccabb54da"
      },
      "source": [
        "!pip install gpustat"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpustat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB)\n",
            "\r\u001b[K     |████▏                           | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 20kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 30kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 40kB 12.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 51kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 61kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from gpustat) (1.15.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat) (7.352.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat) (5.4.8)\n",
            "Collecting blessings>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
            "Building wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-0.6.0-cp37-none-any.whl size=12621 sha256=23a85372349e48e2a7fe13ec6300fa7e5bfa4063088b53028a531e38b601a307\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/b4/d5/fb5b7f1d040f2ff20687e3bad6867d63155dbde5a7c10f4293\n",
            "Successfully built gpustat\n",
            "Installing collected packages: blessings, gpustat\n",
            "Successfully installed blessings-1.7 gpustat-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "T2FPkrxBhTSP",
        "outputId": "d088621e-2245-4a46-813f-63580d08ba3f"
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/43/15d1c4ee4d24fd05ac283e76cc2287a32c02d439e821d8439471f6c869f2/allennlp-2.2.0-py3-none-any.whl (595kB)\n",
            "\r\u001b[K     |▌                               | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 19.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 15.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 40kB 13.2MB/s eta 0:00:01\r\u001b[K     |██▊                             | 51kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 61kB 9.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 81kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 92kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 102kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 112kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 122kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 133kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 143kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 153kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 163kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 174kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 184kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 194kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 204kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 215kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 225kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 235kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 245kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 256kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 266kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 276kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 286kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 296kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 307kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 317kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 327kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 337kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 348kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 358kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 368kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 378kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 389kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 399kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 409kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 419kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 430kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 440kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 450kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 460kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 471kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 481kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 491kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 501kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 512kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 522kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 532kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 542kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 552kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 563kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 573kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 583kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 593kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 604kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision<0.10.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.9.0+cu101)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting boto3<2.0,>=1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/bd/3f9cc87a8faa561903644ec6ef7e7e408ca3640e77c5944124ad6adbaecd/boto3-1.17.39-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 17.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<4.5,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.4.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.1.95)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.10.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.7.0)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Collecting wandb<0.11.0,>=0.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/64/29b77da604e81607e35479bda8c31aabe8911c284fdad488a9030fa4cc0a/wandb-0.10.23-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 16.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.12)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: torch<1.9.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 34.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.10.0,>=0.8.1->allennlp) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->allennlp) (1.15.0)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.5MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.39\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/ad/abdc982cb695a20764df007a2d7cb0ac8964c9591fd014006e40334e4a74/botocore-1.20.39-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 38.9MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<4.5,>=4.1->allennlp) (3.7.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.5,>=4.1->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.5,>=4.1->allennlp) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.5,>=4.1->allennlp) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.5,>=4.1->allennlp) (0.0.43)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (2.8.1)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.13)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (7.1.2)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.12.4)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 39.4MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 42.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (2.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (54.1.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (20.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.9.0,>=1.6.0->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<4.5,>=4.1->allennlp) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.5,>=4.1->allennlp) (2.4.7)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.8MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: jsonnet, subprocess32, pathtools\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388623 sha256=13867525d5a4a0350e1c3cdd0696a5aa2d701a73ff81496a14f6b1541b41ad48\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=4c693d75c6581b6490ea12cfd41b9ac63d33bb58890a374e80690a2155219763\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=16c3791612c04878780dc2e1d7e7445a8379b158336cb0fa25e3c9ae54c1cbcd\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built jsonnet subprocess32 pathtools\n",
            "\u001b[31mERROR: botocore 1.20.39 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, jsonpickle, configparser, subprocess32, pathtools, docker-pycreds, shortuuid, sentry-sdk, smmap, gitdb, GitPython, wandb, jsonnet, tensorboardX, allennlp\n",
            "Successfully installed GitPython-3.1.14 allennlp-2.2.0 boto3-1.17.39 botocore-1.20.39 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-2.0.0 pathtools-0.1.2 s3transfer-0.3.6 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 tensorboardX-2.1 wandb-0.10.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nL1SiDJKgeCR",
        "outputId": "e6a1083e-9e2c-458c-e46c-e56ce7a5f742"
      },
      "source": [
        "!python -u /content/two-are-better-than-one-master/train.py \\\n",
        "     --num_layers 2 \\\n",
        "    --batch_size 12 \\\n",
        "    --evaluate_interval 1000 \\\n",
        "    --dataset CoNLL04 \\\n",
        "    --pretrained_wv /content/two-are-better-than-one-master/wv/glove.6B.100d.conll04.txt  \\\n",
        "    --max_epoches 5000 \\\n",
        "    --max_steps 20000 \\\n",
        "    --model_class JointModel  \\\n",
        "    --model_write_ckpt /content/two-are-better-than-one-master/ckpts/my_model \\\n",
        "    --crf None \\\n",
        "    --optimizer adam \\\n",
        "    --lr 0.001 \\\n",
        "    --tag_form iob2 \\\n",
        "    --cased 0 \\\n",
        "    --token_emb_dim 100 \\\n",
        "    --char_emb_dim 30 \\\n",
        "    --char_encoder lstm \\\n",
        "    --lm_emb_dim 4096 \\\n",
        "    --head_emb_dim 768 \\\n",
        "    --lm_emb_path  /content/two-are-better-than-one-master/wv/albert.conll04_with_heads.pkl  \\\n",
        "    --hidden_dim 200 \\\n",
        "    --ner_tag_vocab_size 32 \\\n",
        "    --re_tag_vocab_size 64 \\\n",
        "    --vocab_size 15000 \\\n",
        "    --dropout 0.5 \\\n",
        "    --grad_period 2\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-28 11:31:07.077866: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "reading pretrained wv from /content/two-are-better-than-one-master/wv/glove.6B.100d.conll04.txt\n",
            "7252it [00:00, 16540.33it/s]\n",
            "reading data..\n",
            "=== start training ===\n",
            "warm up: learning rate was adjusted to 1e-06\n",
            "warm up: learning rate was adjusted to 1.1e-05\n",
            "warm up: learning rate was adjusted to 2.1000000000000002e-05\n",
            "warm up: learning rate was adjusted to 3.1e-05\n",
            "warm up: learning rate was adjusted to 4.1e-05\n",
            "warm up: learning rate was adjusted to 5.1000000000000006e-05\n",
            "warm up: learning rate was adjusted to 6.1e-05\n",
            "warm up: learning rate was adjusted to 7.1e-05\n",
            "warm up: learning rate was adjusted to 8.1e-05\n",
            "warm up: learning rate was adjusted to 9.1e-05\n",
            "g_step 100, step 23, avg_time 1.297, loss:41754.5324\n",
            "warm up: learning rate was adjusted to 0.000101\n",
            "warm up: learning rate was adjusted to 0.000111\n",
            "warm up: learning rate was adjusted to 0.000121\n",
            "warm up: learning rate was adjusted to 0.000131\n",
            "warm up: learning rate was adjusted to 0.000141\n",
            "warm up: learning rate was adjusted to 0.00015099999999999998\n",
            "warm up: learning rate was adjusted to 0.000161\n",
            "warm up: learning rate was adjusted to 0.000171\n",
            "warm up: learning rate was adjusted to 0.00018099999999999998\n",
            "warm up: learning rate was adjusted to 0.000191\n",
            "g_step 200, step 46, avg_time 1.277, loss:1459.6320\n",
            "warm up: learning rate was adjusted to 0.000201\n",
            "warm up: learning rate was adjusted to 0.000211\n",
            "warm up: learning rate was adjusted to 0.000221\n",
            "warm up: learning rate was adjusted to 0.000231\n",
            "warm up: learning rate was adjusted to 0.000241\n",
            "warm up: learning rate was adjusted to 0.000251\n",
            "warm up: learning rate was adjusted to 0.000261\n",
            "warm up: learning rate was adjusted to 0.00027100000000000003\n",
            "warm up: learning rate was adjusted to 0.00028100000000000005\n",
            "warm up: learning rate was adjusted to 0.00029099999999999997\n",
            "g_step 300, step 69, avg_time 1.260, loss:746.3798\n",
            "warm up: learning rate was adjusted to 0.000301\n",
            "warm up: learning rate was adjusted to 0.000311\n",
            "warm up: learning rate was adjusted to 0.000321\n",
            "warm up: learning rate was adjusted to 0.000331\n",
            "warm up: learning rate was adjusted to 0.00034100000000000005\n",
            "warm up: learning rate was adjusted to 0.000351\n",
            "warm up: learning rate was adjusted to 0.000361\n",
            "warm up: learning rate was adjusted to 0.000371\n",
            "warm up: learning rate was adjusted to 0.000381\n",
            "warm up: learning rate was adjusted to 0.000391\n",
            "g_step 400, step 15, avg_time 1.252, loss:521.7469\n",
            "warm up: learning rate was adjusted to 0.00040100000000000004\n",
            "warm up: learning rate was adjusted to 0.000411\n",
            "warm up: learning rate was adjusted to 0.000421\n",
            "warm up: learning rate was adjusted to 0.000431\n",
            "warm up: learning rate was adjusted to 0.000441\n",
            "warm up: learning rate was adjusted to 0.000451\n",
            "warm up: learning rate was adjusted to 0.00046100000000000004\n",
            "warm up: learning rate was adjusted to 0.000471\n",
            "warm up: learning rate was adjusted to 0.000481\n",
            "warm up: learning rate was adjusted to 0.000491\n",
            "g_step 500, step 38, avg_time 1.237, loss:408.3251\n",
            "warm up: learning rate was adjusted to 0.000501\n",
            "warm up: learning rate was adjusted to 0.0005110000000000001\n",
            "warm up: learning rate was adjusted to 0.000521\n",
            "warm up: learning rate was adjusted to 0.000531\n",
            "warm up: learning rate was adjusted to 0.000541\n",
            "warm up: learning rate was adjusted to 0.0005510000000000001\n",
            "warm up: learning rate was adjusted to 0.0005610000000000001\n",
            "warm up: learning rate was adjusted to 0.0005710000000000001\n",
            "warm up: learning rate was adjusted to 0.0005809999999999999\n",
            "warm up: learning rate was adjusted to 0.0005909999999999999\n",
            "g_step 600, step 61, avg_time 1.276, loss:321.3087\n",
            "warm up: learning rate was adjusted to 0.000601\n",
            "warm up: learning rate was adjusted to 0.000611\n",
            "warm up: learning rate was adjusted to 0.000621\n",
            "warm up: learning rate was adjusted to 0.000631\n",
            "warm up: learning rate was adjusted to 0.000641\n",
            "warm up: learning rate was adjusted to 0.000651\n",
            "warm up: learning rate was adjusted to 0.000661\n",
            "warm up: learning rate was adjusted to 0.000671\n",
            "warm up: learning rate was adjusted to 0.0006810000000000001\n",
            "warm up: learning rate was adjusted to 0.0006910000000000001\n",
            "g_step 700, step 7, avg_time 1.292, loss:292.3632\n",
            "warm up: learning rate was adjusted to 0.000701\n",
            "warm up: learning rate was adjusted to 0.0007109999999999999\n",
            "warm up: learning rate was adjusted to 0.000721\n",
            "warm up: learning rate was adjusted to 0.000731\n",
            "warm up: learning rate was adjusted to 0.000741\n",
            "warm up: learning rate was adjusted to 0.000751\n",
            "warm up: learning rate was adjusted to 0.000761\n",
            "warm up: learning rate was adjusted to 0.000771\n",
            "warm up: learning rate was adjusted to 0.000781\n",
            "warm up: learning rate was adjusted to 0.000791\n",
            "g_step 800, step 30, avg_time 1.259, loss:258.7999\n",
            "warm up: learning rate was adjusted to 0.0008010000000000001\n",
            "warm up: learning rate was adjusted to 0.0008110000000000001\n",
            "warm up: learning rate was adjusted to 0.0008210000000000001\n",
            "warm up: learning rate was adjusted to 0.000831\n",
            "warm up: learning rate was adjusted to 0.000841\n",
            "warm up: learning rate was adjusted to 0.000851\n",
            "warm up: learning rate was adjusted to 0.000861\n",
            "warm up: learning rate was adjusted to 0.000871\n",
            "warm up: learning rate was adjusted to 0.0008810000000000001\n",
            "warm up: learning rate was adjusted to 0.000891\n",
            "g_step 900, step 53, avg_time 1.256, loss:215.0784\n",
            "warm up: learning rate was adjusted to 0.000901\n",
            "warm up: learning rate was adjusted to 0.000911\n",
            "warm up: learning rate was adjusted to 0.000921\n",
            "warm up: learning rate was adjusted to 0.0009310000000000001\n",
            "warm up: learning rate was adjusted to 0.0009410000000000001\n",
            "warm up: learning rate was adjusted to 0.000951\n",
            "warm up: learning rate was adjusted to 0.0009609999999999999\n",
            "warm up: learning rate was adjusted to 0.000971\n",
            "warm up: learning rate was adjusted to 0.0009809999999999999\n",
            "warm up: learning rate was adjusted to 0.000991\n",
            "g_step 1000, step 76, avg_time 1.295, loss:208.0090\n",
            "learning rate was adjusted to 0.0009523809523809524\n",
            ">> test entity prec:0.7989, rec:0.8100, f1:0.8044\n",
            ">> test relation prec:0.6091, rec:0.5095, f1:0.5548\n",
            ">> test relation with NER prec:0.5949, rec:0.4976, f1:0.5419\n",
            ">> valid entity prec:0.7935, rec:0.7660, f1:0.7795\n",
            ">> valid relation prec:0.6479, rec:0.5044, f1:0.5672\n",
            ">> valid relation with NER prec:0.6330, rec:0.4927, f1:0.5541\n",
            "new max entity f1 on valid!\n",
            "new max relation f1 on valid!\n",
            "new max relation f1 with NER on valid!\n",
            "new max averaged entity f1 and relation f1 on valid!\n",
            "new max averaged entity f1 and relation f1 with NER on valid!\n",
            "g_step 1100, step 22, avg_time 1.533, loss:175.2726\n",
            "g_step 1200, step 45, avg_time 1.220, loss:159.3087\n",
            "g_step 1300, step 68, avg_time 1.286, loss:145.0403\n",
            "g_step 1400, step 14, avg_time 1.241, loss:116.2901\n",
            "g_step 1500, step 37, avg_time 1.245, loss:108.2327\n",
            "g_step 1600, step 60, avg_time 1.257, loss:87.9456\n",
            "g_step 1700, step 6, avg_time 1.273, loss:85.1521\n",
            "g_step 1800, step 29, avg_time 1.280, loss:82.1009\n",
            "g_step 1900, step 52, avg_time 1.257, loss:67.6773\n",
            "g_step 2000, step 75, avg_time 1.266, loss:63.7735\n",
            "learning rate was adjusted to 0.0009090909090909091\n",
            ">> test entity prec:0.8832, rec:0.8480, f1:0.8652\n",
            ">> test relation prec:0.7069, rec:0.6801, f1:0.6932\n",
            ">> test relation with NER prec:0.6995, rec:0.6730, f1:0.6860\n",
            ">> valid entity prec:0.8551, rec:0.7996, f1:0.8264\n",
            ">> valid relation prec:0.6972, rec:0.6443, f1:0.6697\n",
            ">> valid relation with NER prec:0.6940, rec:0.6414, f1:0.6667\n",
            "new max entity f1 on valid!\n",
            "new max relation f1 on valid!\n",
            "new max relation f1 with NER on valid!\n",
            "new max averaged entity f1 and relation f1 on valid!\n",
            "new max averaged entity f1 and relation f1 with NER on valid!\n",
            "g_step 2100, step 21, avg_time 1.519, loss:50.0134\n",
            "g_step 2200, step 44, avg_time 1.240, loss:49.6638\n",
            "g_step 2300, step 67, avg_time 1.226, loss:43.9413\n",
            "g_step 2400, step 13, avg_time 1.333, loss:44.7409\n",
            "g_step 2500, step 36, avg_time 1.247, loss:39.3245\n",
            "g_step 2600, step 59, avg_time 1.213, loss:36.8959\n",
            "g_step 2700, step 5, avg_time 1.258, loss:37.4951\n",
            "g_step 2800, step 28, avg_time 1.246, loss:31.8550\n",
            "g_step 2900, step 51, avg_time 1.218, loss:31.7984\n",
            "g_step 3000, step 74, avg_time 1.234, loss:29.8445\n",
            "learning rate was adjusted to 0.0008695652173913044\n",
            ">> test entity prec:0.8639, rec:0.8767, f1:0.8703\n",
            ">> test relation prec:0.7348, rec:0.7156, f1:0.7251\n",
            ">> test relation with NER prec:0.7324, rec:0.7133, f1:0.7227\n",
            ">> valid entity prec:0.8534, rec:0.8477, f1:0.8506\n",
            ">> valid relation prec:0.6973, rec:0.6851, f1:0.6912\n",
            ">> valid relation with NER prec:0.6944, rec:0.6822, f1:0.6882\n",
            "new max entity f1 on valid!\n",
            "new max relation f1 on valid!\n",
            "new max relation f1 with NER on valid!\n",
            "new max averaged entity f1 and relation f1 on valid!\n",
            "new max averaged entity f1 and relation f1 with NER on valid!\n",
            "g_step 3100, step 20, avg_time 1.478, loss:26.2654\n",
            "g_step 3200, step 43, avg_time 1.249, loss:26.8541\n",
            "g_step 3300, step 66, avg_time 1.293, loss:24.1775\n",
            "g_step 3400, step 12, avg_time 1.221, loss:26.4899\n",
            "g_step 3500, step 35, avg_time 1.237, loss:25.8237\n",
            "g_step 3600, step 58, avg_time 1.306, loss:29.9563\n",
            "g_step 3700, step 4, avg_time 1.279, loss:21.8391\n",
            "g_step 3800, step 27, avg_time 1.229, loss:20.1356\n",
            "g_step 3900, step 50, avg_time 1.254, loss:20.2906\n",
            "g_step 4000, step 73, avg_time 1.216, loss:20.0330\n",
            "learning rate was adjusted to 0.0008333333333333334\n",
            ">> test entity prec:0.8815, rec:0.8962, f1:0.8888\n",
            ">> test relation prec:0.7903, rec:0.6967, f1:0.7406\n",
            ">> test relation with NER prec:0.7769, rec:0.6848, f1:0.7280\n",
            ">> valid entity prec:0.8655, rec:0.8645, f1:0.8650\n",
            ">> valid relation prec:0.7484, rec:0.6676, f1:0.7057\n",
            ">> valid relation with NER prec:0.7386, rec:0.6589, f1:0.6965\n",
            "new max entity f1 on valid!\n",
            "new max relation f1 on valid!\n",
            "new max relation f1 with NER on valid!\n",
            "new max averaged entity f1 and relation f1 on valid!\n",
            "new max averaged entity f1 and relation f1 with NER on valid!\n",
            "g_step 4100, step 19, avg_time 1.523, loss:17.7037\n",
            "g_step 4200, step 42, avg_time 1.233, loss:15.1863\n",
            "g_step 4300, step 65, avg_time 1.257, loss:16.7188\n",
            "g_step 4400, step 11, avg_time 1.222, loss:14.1200\n",
            "g_step 4500, step 34, avg_time 1.262, loss:14.3763\n",
            "g_step 4600, step 57, avg_time 1.220, loss:14.3363\n",
            "g_step 4700, step 3, avg_time 1.273, loss:14.6545\n",
            "g_step 4800, step 26, avg_time 1.205, loss:13.5543\n",
            "g_step 4900, step 49, avg_time 1.276, loss:14.3023\n",
            "g_step 5000, step 72, avg_time 1.232, loss:15.0746\n",
            "learning rate was adjusted to 0.0008\n",
            ">> test entity prec:0.8925, rec:0.8925, f1:0.8925\n",
            ">> test relation prec:0.7347, rec:0.7417, f1:0.7382\n",
            ">> test relation with NER prec:0.7324, rec:0.7393, f1:0.7358\n",
            ">> valid entity prec:0.8760, rec:0.8544, f1:0.8651\n",
            ">> valid relation prec:0.7039, rec:0.7347, f1:0.7190\n",
            ">> valid relation with NER prec:0.6983, rec:0.7289, f1:0.7133\n",
            "new max entity f1 on valid!\n",
            "new max relation f1 on valid!\n",
            "new max relation f1 with NER on valid!\n",
            "new max averaged entity f1 and relation f1 on valid!\n",
            "new max averaged entity f1 and relation f1 with NER on valid!\n",
            "g_step 5100, step 18, avg_time 1.469, loss:17.6082\n",
            "g_step 5200, step 41, avg_time 1.276, loss:11.6061\n",
            "g_step 5300, step 64, avg_time 1.222, loss:13.2881\n",
            "g_step 5400, step 10, avg_time 1.264, loss:13.5236\n",
            "g_step 5500, step 33, avg_time 1.238, loss:11.2846\n",
            "g_step 5600, step 56, avg_time 1.296, loss:11.2792\n",
            "g_step 5700, step 2, avg_time 1.222, loss:9.2227\n",
            "g_step 5800, step 25, avg_time 1.290, loss:9.8640\n",
            "g_step 5900, step 48, avg_time 1.255, loss:10.8216\n",
            "g_step 6000, step 71, avg_time 1.205, loss:9.6442\n",
            "learning rate was adjusted to 0.0007692307692307692\n",
            ">> test entity prec:0.8871, rec:0.8814, f1:0.8842\n",
            ">> test relation prec:0.7769, rec:0.6682, f1:0.7185\n",
            ">> test relation with NER prec:0.7741, rec:0.6659, f1:0.7159\n",
            ">> valid entity prec:0.8824, rec:0.8567, f1:0.8693\n",
            ">> valid relation prec:0.7608, rec:0.6676, f1:0.7112\n",
            ">> valid relation with NER prec:0.7608, rec:0.6676, f1:0.7112\n",
            "new max entity f1 on valid!\n",
            "new max averaged entity f1 and relation f1 with NER on valid!\n",
            "g_step 6100, step 17, avg_time 1.475, loss:9.0464\n",
            "g_step 6200, step 40, avg_time 1.273, loss:10.5274\n",
            "g_step 6300, step 63, avg_time 1.269, loss:10.8205\n",
            "g_step 6400, step 9, avg_time 1.267, loss:8.9593\n",
            "g_step 6500, step 32, avg_time 1.279, loss:8.3417\n",
            "g_step 6600, step 55, avg_time 1.274, loss:10.4190\n",
            "g_step 6700, step 1, avg_time 1.273, loss:7.2425\n",
            "g_step 6800, step 24, avg_time 1.207, loss:6.0571\n",
            "g_step 6900, step 47, avg_time 1.319, loss:8.2324\n",
            "g_step 7000, step 70, avg_time 1.178, loss:6.1774\n",
            "learning rate was adjusted to 0.0007407407407407407\n",
            ">> test entity prec:0.8711, rec:0.8832, f1:0.8771\n",
            ">> test relation prec:0.7690, rec:0.7180, f1:0.7426\n",
            ">> test relation with NER prec:0.7665, rec:0.7156, f1:0.7402\n",
            ">> valid entity prec:0.8771, rec:0.8634, f1:0.8702\n",
            ">> valid relation prec:0.7743, rec:0.7201, f1:0.7462\n",
            ">> valid relation with NER prec:0.7743, rec:0.7201, f1:0.7462\n",
            "new max entity f1 on valid!\n",
            "new max relation f1 on valid!\n",
            "new max relation f1 with NER on valid!\n",
            "new max averaged entity f1 and relation f1 on valid!\n",
            "new max averaged entity f1 and relation f1 with NER on valid!\n",
            "g_step 7100, step 16, avg_time 1.544, loss:8.4072\n",
            "g_step 7200, step 39, avg_time 1.218, loss:8.4810\n",
            "g_step 7300, step 62, avg_time 1.288, loss:6.6539\n",
            "g_step 7400, step 8, avg_time 1.256, loss:7.4500\n",
            "g_step 7500, step 31, avg_time 1.218, loss:8.2166\n",
            "g_step 7600, step 54, avg_time 1.317, loss:10.0745\n",
            "g_step 7700, step 77, avg_time 1.254, loss:5.3794\n",
            "g_step 7800, step 23, avg_time 1.229, loss:8.4759\n",
            "g_step 7900, step 46, avg_time 1.196, loss:6.2303\n",
            "g_step 8000, step 69, avg_time 1.300, loss:5.5710\n",
            "learning rate was adjusted to 0.0007142857142857144\n",
            ">> test entity prec:0.8964, rec:0.8897, f1:0.8930\n",
            ">> test relation prec:0.7619, rec:0.6825, f1:0.7200\n",
            ">> test relation with NER prec:0.7619, rec:0.6825, f1:0.7200\n",
            ">> valid entity prec:0.8899, rec:0.8690, f1:0.8793\n",
            ">> valid relation prec:0.7883, rec:0.7055, f1:0.7446\n",
            ">> valid relation with NER prec:0.7850, rec:0.7026, f1:0.7415\n",
            "new max entity f1 on valid!\n",
            "new max averaged entity f1 and relation f1 on valid!\n",
            "new max averaged entity f1 and relation f1 with NER on valid!\n",
            "g_step 8100, step 15, avg_time 1.526, loss:7.9647\n",
            "g_step 8200, step 38, avg_time 1.198, loss:6.4333\n",
            "g_step 8300, step 61, avg_time 1.269, loss:7.5243\n",
            "g_step 8400, step 7, avg_time 1.221, loss:7.1718\n",
            "g_step 8500, step 30, avg_time 1.214, loss:6.4813\n",
            "g_step 8600, step 53, avg_time 1.232, loss:7.5125\n",
            "g_step 8700, step 76, avg_time 1.273, loss:6.3650\n",
            "g_step 8800, step 22, avg_time 1.241, loss:5.4456\n",
            "g_step 8900, step 45, avg_time 1.269, loss:6.3159\n",
            "g_step 9000, step 68, avg_time 1.226, loss:5.2749\n",
            "learning rate was adjusted to 0.0006896551724137932\n",
            ">> test entity prec:0.8918, rec:0.8934, f1:0.8926\n",
            ">> test relation prec:0.7652, rec:0.7180, f1:0.7408\n",
            ">> test relation with NER prec:0.7652, rec:0.7180, f1:0.7408\n",
            ">> valid entity prec:0.8857, rec:0.8679, f1:0.8767\n",
            ">> valid relation prec:0.7572, rec:0.6910, f1:0.7226\n",
            ">> valid relation with NER prec:0.7572, rec:0.6910, f1:0.7226\n",
            "g_step 9100, step 14, avg_time 1.442, loss:6.1419\n",
            "g_step 9200, step 37, avg_time 1.291, loss:6.4836\n",
            "g_step 9300, step 60, avg_time 1.251, loss:5.2697\n",
            "g_step 9400, step 6, avg_time 1.233, loss:6.4986\n",
            "g_step 9500, step 29, avg_time 1.312, loss:6.3163\n",
            "g_step 9600, step 52, avg_time 1.215, loss:4.4142\n",
            "g_step 9700, step 75, avg_time 1.257, loss:5.8904\n",
            "g_step 9800, step 21, avg_time 1.276, loss:5.7807\n",
            "g_step 9900, step 44, avg_time 1.247, loss:4.2606\n",
            "g_step 10000, step 67, avg_time 1.277, loss:6.1079\n",
            "learning rate was adjusted to 0.0006666666666666666\n",
            ">> test entity prec:0.8981, rec:0.8906, f1:0.8944\n",
            ">> test relation prec:0.7757, rec:0.6967, f1:0.7341\n",
            ">> test relation with NER prec:0.7731, rec:0.6943, f1:0.7316\n",
            ">> valid entity prec:0.8799, rec:0.8611, f1:0.8704\n",
            ">> valid relation prec:0.7573, rec:0.6822, f1:0.7178\n",
            ">> valid relation with NER prec:0.7508, rec:0.6764, f1:0.7117\n",
            "g_step 10100, step 13, avg_time 1.454, loss:7.6692\n",
            "g_step 10200, step 36, avg_time 1.255, loss:5.7866\n",
            "g_step 10300, step 59, avg_time 1.278, loss:6.8082\n",
            "g_step 10400, step 5, avg_time 1.223, loss:5.2529\n",
            "g_step 10500, step 28, avg_time 1.252, loss:4.4001\n",
            "g_step 10600, step 51, avg_time 1.193, loss:4.8094\n",
            "g_step 10700, step 74, avg_time 1.318, loss:3.1996\n",
            "g_step 10800, step 20, avg_time 1.197, loss:4.4736\n",
            "g_step 10900, step 43, avg_time 1.282, loss:6.1808\n",
            "g_step 11000, step 66, avg_time 1.233, loss:6.3700\n",
            "learning rate was adjusted to 0.0006451612903225806\n",
            ">> test entity prec:0.9064, rec:0.8888, f1:0.8975\n",
            ">> test relation prec:0.7726, rec:0.7085, f1:0.7392\n",
            ">> test relation with NER prec:0.7700, rec:0.7062, f1:0.7367\n",
            ">> valid entity prec:0.8836, rec:0.8589, f1:0.8711\n",
            ">> valid relation prec:0.7645, rec:0.7289, f1:0.7463\n",
            ">> valid relation with NER prec:0.7615, rec:0.7259, f1:0.7433\n",
            "new max relation f1 on valid!\n",
            "g_step 11100, step 12, avg_time 1.446, loss:4.1869\n",
            "g_step 11200, step 35, avg_time 1.284, loss:4.3012\n",
            "g_step 11300, step 58, avg_time 1.227, loss:2.3777\n",
            "g_step 11400, step 4, avg_time 1.232, loss:5.6095\n",
            "g_step 11500, step 27, avg_time 1.252, loss:6.1167\n",
            "g_step 11600, step 50, avg_time 1.251, loss:4.5150\n",
            "g_step 11700, step 73, avg_time 1.241, loss:3.3194\n",
            "g_step 11800, step 19, avg_time 1.214, loss:4.8645\n",
            "g_step 11900, step 42, avg_time 1.277, loss:4.4896\n",
            "g_step 12000, step 65, avg_time 1.221, loss:3.9000\n",
            "learning rate was adjusted to 0.000625\n",
            ">> test entity prec:0.8965, rec:0.8906, f1:0.8935\n",
            ">> test relation prec:0.7680, rec:0.7062, f1:0.7358\n",
            ">> test relation with NER prec:0.7629, rec:0.7014, f1:0.7309\n",
            ">> valid entity prec:0.8810, rec:0.8701, f1:0.8755\n",
            ">> valid relation prec:0.7586, rec:0.7055, f1:0.7311\n",
            ">> valid relation with NER prec:0.7586, rec:0.7055, f1:0.7311\n",
            "g_step 12100, step 11, avg_time 1.462, loss:2.9609\n",
            "g_step 12200, step 34, avg_time 1.289, loss:2.0860\n",
            "g_step 12300, step 57, avg_time 1.239, loss:4.0101\n",
            "g_step 12400, step 3, avg_time 1.220, loss:4.4184\n",
            "g_step 12500, step 26, avg_time 1.282, loss:3.9405\n",
            "g_step 12600, step 49, avg_time 1.227, loss:5.1836\n",
            "g_step 12700, step 72, avg_time 1.233, loss:2.4658\n",
            "g_step 12800, step 18, avg_time 1.313, loss:3.4020\n",
            "g_step 12900, step 41, avg_time 1.251, loss:3.5626\n",
            "g_step 13000, step 64, avg_time 1.191, loss:4.4610\n",
            "learning rate was adjusted to 0.0006060606060606061\n",
            ">> test entity prec:0.9058, rec:0.8916, f1:0.8986\n",
            ">> test relation prec:0.7738, rec:0.7133, f1:0.7423\n",
            ">> test relation with NER prec:0.7686, rec:0.7085, f1:0.7374\n",
            ">> valid entity prec:0.8932, rec:0.8712, f1:0.8821\n",
            ">> valid relation prec:0.7607, rec:0.7230, f1:0.7414\n",
            ">> valid relation with NER prec:0.7607, rec:0.7230, f1:0.7414\n",
            "new max entity f1 on valid!\n",
            "new max averaged entity f1 and relation f1 with NER on valid!\n",
            "g_step 13100, step 10, avg_time 1.494, loss:2.3734\n",
            "g_step 13200, step 33, avg_time 1.294, loss:4.1922\n",
            "g_step 13300, step 56, avg_time 1.205, loss:5.6125\n",
            "g_step 13400, step 2, avg_time 1.256, loss:3.0303\n",
            "g_step 13500, step 25, avg_time 1.240, loss:2.7517\n",
            "g_step 13600, step 48, avg_time 1.275, loss:3.0448\n",
            "g_step 13700, step 71, avg_time 1.254, loss:4.4054\n",
            "g_step 13800, step 17, avg_time 1.195, loss:2.9516\n",
            "g_step 13900, step 40, avg_time 1.271, loss:4.5079\n",
            "g_step 14000, step 63, avg_time 1.288, loss:5.1557\n",
            "learning rate was adjusted to 0.0005882352941176471\n",
            ">> test entity prec:0.8839, rec:0.8962, f1:0.8900\n",
            ">> test relation prec:0.7564, rec:0.6991, f1:0.7266\n",
            ">> test relation with NER prec:0.7513, rec:0.6943, f1:0.7217\n",
            ">> valid entity prec:0.8825, rec:0.8746, f1:0.8785\n",
            ">> valid relation prec:0.7508, rec:0.7289, f1:0.7396\n",
            ">> valid relation with NER prec:0.7477, rec:0.7259, f1:0.7367\n",
            "g_step 14100, step 9, avg_time 1.460, loss:3.2164\n",
            "g_step 14200, step 32, avg_time 1.210, loss:3.3248\n",
            "g_step 14300, step 55, avg_time 1.261, loss:2.9862\n",
            "g_step 14400, step 1, avg_time 1.273, loss:2.9297\n",
            "g_step 14500, step 24, avg_time 1.258, loss:2.7802\n",
            "g_step 14600, step 47, avg_time 1.253, loss:3.9594\n",
            "g_step 14700, step 70, avg_time 1.286, loss:2.6247\n",
            "g_step 14800, step 16, avg_time 1.247, loss:4.4812\n",
            "g_step 14900, step 39, avg_time 1.217, loss:3.9525\n",
            "g_step 15000, step 62, avg_time 1.321, loss:4.5296\n",
            "learning rate was adjusted to 0.0005714285714285715\n",
            ">> test entity prec:0.9030, rec:0.8971, f1:0.9000\n",
            ">> test relation prec:0.7296, rec:0.7417, f1:0.7356\n",
            ">> test relation with NER prec:0.7226, rec:0.7346, f1:0.7286\n",
            ">> valid entity prec:0.8890, rec:0.8791, f1:0.8840\n",
            ">> valid relation prec:0.7123, rec:0.7580, f1:0.7345\n",
            ">> valid relation with NER prec:0.7096, rec:0.7551, f1:0.7316\n",
            "new max entity f1 on valid!\n",
            "g_step 15100, step 8, avg_time 1.446, loss:4.5487\n",
            "g_step 15200, step 31, avg_time 1.256, loss:2.6143\n",
            "g_step 15300, step 54, avg_time 1.251, loss:1.3449\n",
            "g_step 15400, step 77, avg_time 1.240, loss:4.0867\n",
            "g_step 15500, step 23, avg_time 1.223, loss:4.1572\n",
            "g_step 15600, step 46, avg_time 1.287, loss:5.3043\n",
            "g_step 15700, step 69, avg_time 1.234, loss:3.6421\n",
            "g_step 15800, step 15, avg_time 1.278, loss:3.1903\n",
            "g_step 15900, step 38, avg_time 1.265, loss:3.6824\n",
            "g_step 16000, step 61, avg_time 1.231, loss:2.1020\n",
            "learning rate was adjusted to 0.0005555555555555556\n",
            ">> test entity prec:0.8927, rec:0.9018, f1:0.8972\n",
            ">> test relation prec:0.7525, rec:0.7204, f1:0.7361\n",
            ">> test relation with NER prec:0.7500, rec:0.7180, f1:0.7337\n",
            ">> valid entity prec:0.8773, rec:0.8723, f1:0.8748\n",
            ">> valid relation prec:0.7591, rec:0.7259, f1:0.7422\n",
            ">> valid relation with NER prec:0.7591, rec:0.7259, f1:0.7422\n",
            "g_step 16100, step 7, avg_time 1.513, loss:2.2796\n",
            "g_step 16200, step 30, avg_time 1.267, loss:2.9867\n",
            "g_step 16300, step 53, avg_time 1.230, loss:1.6619\n",
            "g_step 16400, step 76, avg_time 1.248, loss:3.3639\n",
            "g_step 16500, step 22, avg_time 1.254, loss:3.9641\n",
            "g_step 16600, step 45, avg_time 1.261, loss:2.3976\n",
            "g_step 16700, step 68, avg_time 1.243, loss:2.2960\n",
            "g_step 16800, step 14, avg_time 1.208, loss:3.0115\n",
            "g_step 16900, step 37, avg_time 1.257, loss:3.2827\n",
            "g_step 17000, step 60, avg_time 1.247, loss:2.5381\n",
            "learning rate was adjusted to 0.0005405405405405405\n",
            ">> test entity prec:0.8945, rec:0.8962, f1:0.8954\n",
            ">> test relation prec:0.7420, rec:0.7156, f1:0.7286\n",
            ">> test relation with NER prec:0.7371, rec:0.7109, f1:0.7238\n",
            ">> valid entity prec:0.8904, rec:0.8735, f1:0.8819\n",
            ">> valid relation prec:0.7463, rec:0.7289, f1:0.7375\n",
            ">> valid relation with NER prec:0.7463, rec:0.7289, f1:0.7375\n",
            "g_step 17100, step 6, avg_time 1.509, loss:3.4108\n",
            "g_step 17200, step 29, avg_time 1.274, loss:2.7668\n",
            "g_step 17300, step 52, avg_time 1.242, loss:1.9044\n",
            "g_step 17400, step 75, avg_time 1.215, loss:2.0930\n",
            "g_step 17500, step 21, avg_time 1.265, loss:2.7672\n",
            "g_step 17600, step 44, avg_time 1.243, loss:2.5312\n",
            "g_step 17700, step 67, avg_time 1.190, loss:2.8815\n",
            "g_step 17800, step 13, avg_time 1.245, loss:3.7018\n",
            "g_step 17900, step 36, avg_time 1.254, loss:2.6657\n",
            "g_step 18000, step 59, avg_time 1.203, loss:2.0261\n",
            "learning rate was adjusted to 0.0005263157894736842\n",
            ">> test entity prec:0.9006, rec:0.8814, f1:0.8909\n",
            ">> test relation prec:0.7720, rec:0.7062, f1:0.7376\n",
            ">> test relation with NER prec:0.7694, rec:0.7038, f1:0.7351\n",
            ">> valid entity prec:0.8852, rec:0.8634, f1:0.8741\n",
            ">> valid relation prec:0.7439, rec:0.7114, f1:0.7273\n",
            ">> valid relation with NER prec:0.7439, rec:0.7114, f1:0.7273\n",
            "g_step 18100, step 5, avg_time 1.482, loss:3.2912\n",
            "g_step 18200, step 28, avg_time 1.218, loss:3.2634\n",
            "g_step 18300, step 51, avg_time 1.253, loss:2.4750\n",
            "g_step 18400, step 74, avg_time 1.235, loss:1.6006\n",
            "g_step 18500, step 20, avg_time 1.219, loss:3.1027\n",
            "g_step 18600, step 43, avg_time 1.216, loss:2.8314\n",
            "g_step 18700, step 66, avg_time 1.310, loss:2.1665\n",
            "g_step 18800, step 12, avg_time 1.186, loss:2.4068\n",
            "g_step 18900, step 35, avg_time 1.278, loss:1.5082\n",
            "g_step 19000, step 58, avg_time 1.211, loss:2.5765\n",
            "learning rate was adjusted to 0.0005128205128205128\n",
            ">> test entity prec:0.8925, rec:0.8925, f1:0.8925\n",
            ">> test relation prec:0.7230, rec:0.7299, f1:0.7264\n",
            ">> test relation with NER prec:0.7183, rec:0.7251, f1:0.7217\n",
            ">> valid entity prec:0.8869, rec:0.8779, f1:0.8824\n",
            ">> valid relation prec:0.7263, rec:0.7580, f1:0.7418\n",
            ">> valid relation with NER prec:0.7263, rec:0.7580, f1:0.7418\n",
            "new max averaged entity f1 and relation f1 on valid!\n",
            "new max averaged entity f1 and relation f1 with NER on valid!\n",
            "g_step 19100, step 4, avg_time 1.518, loss:2.4911\n",
            "g_step 19200, step 27, avg_time 1.224, loss:3.4151\n",
            "g_step 19300, step 50, avg_time 1.202, loss:3.7515\n",
            "g_step 19400, step 73, avg_time 1.269, loss:2.0566\n",
            "g_step 19500, step 19, avg_time 1.221, loss:1.5253\n",
            "g_step 19600, step 42, avg_time 1.194, loss:3.0256\n",
            "g_step 19700, step 65, avg_time 1.254, loss:2.8036\n",
            "g_step 19800, step 11, avg_time 1.240, loss:2.8157\n",
            "g_step 19900, step 34, avg_time 1.200, loss:1.8775\n",
            "g_step 20000, step 57, avg_time 1.313, loss:3.1393\n",
            "learning rate was adjusted to 0.0005\n",
            ">> test entity prec:0.8902, rec:0.8869, f1:0.8886\n",
            ">> test relation prec:0.7482, rec:0.7251, f1:0.7365\n",
            ">> test relation with NER prec:0.7433, rec:0.7204, f1:0.7316\n",
            ">> valid entity prec:0.8747, rec:0.8600, f1:0.8673\n",
            ">> valid relation prec:0.7330, rec:0.7522, f1:0.7424\n",
            ">> valid relation with NER prec:0.7330, rec:0.7522, f1:0.7424\n",
            "reach max_steps, stop training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYUOZqE5hZr3"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/two-are-better-than-one-master')\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsJr_ceLkN1e"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "from utils import *\n",
        "from data import *\n",
        "from models import *\n",
        "\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "torch.set_num_threads(4)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27QTIL-2k49N"
      },
      "source": [
        "args = {\n",
        "    'model_read_ckpt': '/content/two-are-better-than-one-master/ckpts/my_model',\n",
        "    'lm_emb_path': 'albert-xxlarge-v1', # language model name\n",
        "    'pretrained_wv': '/content/two-are-better-than-one-master/wv/glove.6B.100d.conll04.txt', # original GloVe embeddings\n",
        "    'vocab_size': 400100, # GloVe contains 400,000 words\n",
        "    'device': 'cpu',\n",
        "}"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Muo_aoaDlEuX"
      },
      "source": [
        "if args['device'] is not None and args['device'] != 'cpu':\n",
        "    torch.cuda.set_device(args['device'])\n",
        "elif args['device'] is None:\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_idx, gpu_mem = set_max_available_gpu()\n",
        "        args['device'] = f\"cuda:{gpu_idx}\"\n",
        "    else:\n",
        "        args['device'] = \"cpu\""
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7LwYMxAle_M"
      },
      "source": [
        "# Load the config file of trained ckpt\n",
        "with open(args['model_read_ckpt']+'.json', 'r') as f:\n",
        "    config = Config(**json.load(f))\n",
        "    \n",
        "    # load language model to dynamically calculate the contextualized word embeddings\n",
        "    config.lm_emb_path = args['lm_emb_path']\n",
        "    # assign device\n",
        "    config.device = args['device']"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_oEQScCClhQN",
        "outputId": "d016b104-b319-4106-9e86-0e1bba6f071b"
      },
      "source": [
        "model = JointModel(config)\n",
        "model.load_ckpt(args['model_read_ckpt'])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "albert-xxlarge-v1 is not file, try load as bert model.\n",
            "albert-xxlarge-v1 loaded successfully.\n",
            "Note it only supports default options now, i.e.: \n",
            "  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation=\"mean\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hNeeC3q7l26J",
        "outputId": "3d6e302f-75b2-4e85-be58-8cff6b8f6424"
      },
      "source": [
        "# Load full GloVe embeddings \n",
        "# *this is needed when training on the reduced version of GloVe word vectors \n",
        "# *you can comment this block if the OOV problem is not very serious.\n",
        "_w = model.token_embedding.token_embedding.weight \n",
        "_w_data = _w.data\n",
        "_w.data = torch.zeros([args['vocab_size'], config.token_emb_dim], dtype=_w.dtype, device=_w.device)\n",
        "model.token_embedding.load_pretrained(args['pretrained_wv'], freeze=True)\n",
        "_w.data[:len(_w_data)] = _w_data"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7252it [00:00, 16418.07it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUAbDlDtl76T"
      },
      "source": [
        "# Now predict on custom text\n",
        "rets = model.predict_step({\n",
        "    'tokens': [['My', 'name', 'is', 'Jackson', ',', 'and', 'I', 'live', 'in', 'Berlin', '.'],]\n",
        "})\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "n6njPzqZmFQA",
        "outputId": "f90f07a5-d3c1-48b9-a370-bbfedf4b527b"
      },
      "source": [
        "rets['relation_preds']"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{(3, 4, 9, 10, 'Live_In')}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvLZYSMsmQEy"
      },
      "source": [
        ""
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FbI68ffHmHtJ",
        "outputId": "76cf7852-3673-4137-b412-30630c367b33"
      },
      "source": [
        "rets['entity_preds']"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(3, 4, 'Peop'), (9, 10, 'Loc')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SdA6uRTIp7-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}